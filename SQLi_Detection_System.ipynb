{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuKsk1CDq8/gJnNJnW0BTZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangsen83/SQLi/blob/main/SQLi_Detection_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Block 1: Library Imports and Dataset Integration**\n",
        "This block handles the environment setup and merges your three specific Kaggle datasets into a single heterogeneous corpus."
      ],
      "metadata": {
        "id": "MrGhtMesWfgv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx6JfnG5JGsn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import re\n",
        "import urllib.parse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# LOAD & MERGE DATA\n",
        "print(\"ðŸ“‚ Loading files...\")\n",
        "try:\n",
        "    # Load the specific datasets\n",
        "    df1 = pd.read_csv(\"sqli.csv\", encoding='utf-16', on_bad_lines='skip')\n",
        "    df2 = pd.read_csv(\"sqliv2.csv\", encoding='utf-16', on_bad_lines='skip', engine='python')\n",
        "    df3 = pd.read_csv(\"SQLiV3.csv\", encoding='utf-8', on_bad_lines='skip')\n",
        "\n",
        "    # 1. SHOW INDIVIDUAL ROW COUNTS\n",
        "    print(f\"   - sqli.csv rows:    {len(df1)}\")\n",
        "    print(f\"   - sqliv2.csv rows:  {len(df2)}\")\n",
        "    print(f\"   - SQLiV3.csv rows:  {len(df3)}\")\n",
        "\n",
        "    total_raw = len(df1) + len(df2) + len(df3)\n",
        "    print(f\"   ---------------------------\")\n",
        "    print(f\"   - Total Raw Rows:   {total_raw}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ ERROR: Files not found! Please upload sqli.csv, sqliv2.csv, and SQLiV3.csv.\")\n",
        "    raise\n",
        "\n",
        "# Function to standardize column names\n",
        "def fix_columns(df_in):\n",
        "    df = df_in.copy()\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    rename_map = {\n",
        "        'sentence': 'query', 'text': 'query', 'sql': 'query', 'command': 'query',\n",
        "        'class': 'label', 'value': 'label', 'attack': 'label',\n",
        "        'unnamed: 1': 'query', 'Ã¿Ã¾s': 'label'\n",
        "    }\n",
        "    df.rename(columns=rename_map, inplace=True)\n",
        "    if 'query' in df.columns and 'label' in df.columns:\n",
        "        return df[['query', 'label']]\n",
        "    return pd.DataFrame(columns=['query', 'label'])\n",
        "\n",
        "# Merge and Clean\n",
        "df = pd.concat([fix_columns(df1), fix_columns(df2), fix_columns(df3)], ignore_index=True)\n",
        "\n",
        "#CALCULATE MISSED ROWS (Cleaning Process)\n",
        "initial_merged_count = len(df)\n",
        "df.dropna(subset=['query', 'label'], inplace=True)\n",
        "df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
        "df.dropna(subset=['label'], inplace=True)\n",
        "final_count = len(df)\n",
        "dropped_rows = total_raw - final_count\n",
        "\n",
        "print(f\"   ---------------------------\")\n",
        "print(f\"âœ… Data Merged & Cleaned.\")\n",
        "print(f\"   - Final Rows Used:  {final_count}\")\n",
        "print(f\"   - Rows Missed/Dropped: {dropped_rows} (due to empty or malformed data)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Block 2: Semantic Generalization and Preprocessing**"
      ],
      "metadata": {
        "id": "XCX1LT0uWocC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def advanced_clean(text):\n",
        "    # 1. Protocol Decoding\n",
        "    try:\n",
        "        text = urllib.parse.unquote(text) # Decode %20, %27 etc.\n",
        "    except:\n",
        "        pass\n",
        "    # 2. Numeric Abstraction\n",
        "    text = re.sub(r'\\d+', 'NUM', text) # Replace specific numbers with 'NUM'\n",
        "    return text.lower()\n",
        "\n",
        "print(\"âš™ï¸ Applying Advanced Cleaning (Semantic Generalization)...\")\n",
        "df['clean_query'] = df['query'].apply(advanced_clean)\n",
        "\n",
        "# Split Data (70% Train, 15% Val, 15% Test)\n",
        "X = df['clean_query']\n",
        "y = df['label'].values\n",
        "X_train_txt, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val_txt, X_test_txt, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# A) Vectorize for Random Forest (TF-IDF)\n",
        "print(\"ðŸ”¹ Vectorizing for Random Forest...\")\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), analyzer='char')\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_txt)\n",
        "X_test_tfidf = tfidf.transform(X_test_txt)\n",
        "\n",
        "# B) Tokenize for CNN-LSTM (Sequence)\n",
        "print(\"ðŸ”¹ Tokenizing for CNN-LSTM...\")\n",
        "max_words = 10000\n",
        "max_len = 200\n",
        "tokenizer = Tokenizer(num_words=max_words, char_level=True) # Character-level is key for SQLi\n",
        "tokenizer.fit_on_texts(X_train_txt)\n",
        "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train_txt), maxlen=max_len)\n",
        "X_val_seq = pad_sequences(tokenizer.texts_to_sequences(X_val_txt), maxlen=max_len)\n",
        "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test_txt), maxlen=max_len)\n",
        "print(\"âœ… Preprocessing Complete!\")"
      ],
      "metadata": {
        "id": "-OT_Dr2UJPHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Block 3: Model Training (RF & CNN-LSTM)**\n",
        "This block builds the hybrid architecture and trains both models."
      ],
      "metadata": {
        "id": "IyCbiOywWt7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Random Forest Training\n",
        "print(\"ðŸŒ² Training Random Forest (Baseline)...\")\n",
        "start_rf = time.time()\n",
        "rf_model = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "rf_duration = time.time() - start_rf\n",
        "print(f\"âœ… RF Trained in {rf_duration:.2f}s\")\n",
        "\n",
        "# 2. CNN-LSTM Training\n",
        "print(\"\\nðŸ§  Training CNN-LSTM (Deep Learning)...\")\n",
        "# Build Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=32, input_length=max_len))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# --- START TIMER FOR CNN-LSTM ---\n",
        "print(\"   (Starting training timer...)\")\n",
        "start_dl = time.time()\n",
        "\n",
        "# Training loop\n",
        "model.fit(X_train_seq, y_train, validation_data=(X_val_seq, y_val), epochs=5, batch_size=64, verbose=1)\n",
        "\n",
        "# --- END TIMER FOR CNN-LSTM ---\n",
        "end_dl = time.time()\n",
        "dl_duration = end_dl - start_dl\n",
        "\n",
        "print(f\"âœ… CNN-LSTM Trained in {dl_duration:.2f}s\")"
      ],
      "metadata": {
        "id": "I-xhTDpeJTfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Block 4: Comprehensive Evaluation**\n",
        "This block consolidates all metrics (Precision, Recall, F1, FPR, and Latency) into a single report."
      ],
      "metadata": {
        "id": "PRkrAEDwW1Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, X, y, name, is_dl=False):\n",
        "    start = time.time()\n",
        "    if is_dl:\n",
        "        probs = model.predict(X, verbose=0)\n",
        "        preds = (probs > 0.5).astype(int).flatten()\n",
        "    else:\n",
        "        preds = model.predict(X)\n",
        "\n",
        "    # Latency Calculation\n",
        "    lat = (time.time() - start) / X.shape[0] * 1000 # ms per query\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(y, preds)\n",
        "    prec = precision_score(y, preds)\n",
        "    rec = recall_score(y, preds)\n",
        "    f1 = f1_score(y, preds)\n",
        "\n",
        "    # False Positive Rate\n",
        "    cm = confusion_matrix(y, preds)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    fpr = fp / (fp + tn) * 100\n",
        "\n",
        "    print(f\"\\n--- {name} Results ---\")\n",
        "    print(f\"Accuracy:      {acc*100:.2f}%\")\n",
        "    print(f\"Precision:     {prec*100:.2f}%\")\n",
        "    print(f\"Recall:        {rec*100:.2f}%\")\n",
        "    print(f\"F1-Score:      {f1*100:.2f}%\")\n",
        "    print(f\"FPR:           {fpr:.2f}% (Lower is better)\")\n",
        "    print(f\"Latency:       {lat:.4f} ms/query\")\n",
        "    return cm, fpr, lat, acc\n",
        "\n",
        "# Run Standard Tests\n",
        "print(\"ðŸ“Š --- FINAL STANDARD RESULTS ---\")\n",
        "cm_rf, fpr_rf, lat_rf, acc_rf = evaluate(rf_model, X_test_tfidf, y_test, \"Random Forest\")\n",
        "cm_dl, fpr_dl, lat_dl, acc_dl = evaluate(model, X_test_seq, y_test, \"CNN-LSTM\", is_dl=True)"
      ],
      "metadata": {
        "id": "sHPfwBSiJVNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Block 5: Adversarial Stress Test (Mutation Engine)**\n",
        "To evaluate the Adversarial Robustness of both models by subjecting them to obfuscated attacks that were not seen during training."
      ],
      "metadata": {
        "id": "HhYSTe_6XAMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "print(\"\\nâš”ï¸ --- STARTING STRESS TEST (Mutation Engine V2) ---\")\n",
        "\n",
        "# 1. Define Mutation Logic\n",
        "def mutate_payload(payload):\n",
        "    mutation = random.choice(['case', 'comment', 'encoding', 'mixed'])\n",
        "    if mutation == 'case':\n",
        "        # Randomly capitalize letters (SeLeCt)\n",
        "        return ''.join(random.choice([k.upper(), k.lower()]) for k in payload)\n",
        "    elif mutation == 'comment':\n",
        "        # Break keywords with comments (SEL/**/ECT)\n",
        "        return payload.replace(\" \", \"/**/\")\n",
        "    elif mutation == 'encoding':\n",
        "        # URL Encode the whole string\n",
        "        return urllib.parse.quote(payload)\n",
        "    elif mutation == 'mixed':\n",
        "        # Mix comments and encoding\n",
        "        return urllib.parse.quote(payload.replace(\" \", \"/**/\"))\n",
        "    return payload\n",
        "\n",
        "# 2. Generate 100 Attacks (EXPANDED LIST)\n",
        "# We use more varieties to stop the models from tying on simple queries\n",
        "base_attacks = [\n",
        "    \"' OR 1=1 --\",\n",
        "    \"UNION SELECT user, password FROM users\",\n",
        "    \"admin' --\",\n",
        "    \"AND 1=1\",\n",
        "    \"ORDER BY 1--\",\n",
        "    \"SELECT * FROM v$version\",\n",
        "    \"Waitfor delay '0:0:5'\",\n",
        "    \"1' OR '1'='1\",\n",
        "    \"admin'/*\",\n",
        "    \"UNION ALL SELECT 1,2,3,4,5,6,7\"\n",
        "]\n",
        "\n",
        "# Generate 100 random attacks from this larger list\n",
        "adversarial_data = [mutate_payload(random.choice(base_attacks)) for _ in range(100)]\n",
        "\n",
        "# 3. Process & Predict\n",
        "# Apply the same cleaning the models saw during training\n",
        "adv_txt = [advanced_clean(x) for x in adversarial_data]\n",
        "\n",
        "# A) Random Forest Prediction\n",
        "# We expect RF to struggle because TF-IDF hates broken words like \"sel/**/ect\"\n",
        "rf_preds = rf_model.predict(tfidf.transform(adv_txt))\n",
        "rf_missed = int((1 - rf_preds).sum()) # Cast to int to remove decimal\n",
        "\n",
        "# B) CNN-LSTM Prediction\n",
        "# We expect CNN to do better because it reads character-by-character (s-e-l...)\n",
        "adv_seq = pad_sequences(tokenizer.texts_to_sequences(adv_txt), maxlen=max_len)\n",
        "dl_probs = model.predict(adv_seq, verbose=0)\n",
        "dl_missed = int((1 - (dl_probs > 0.5).astype(int).flatten()).sum()) # Cast to int\n",
        "\n",
        "# 4. Final Output\n",
        "print(f\"ðŸ›¡ï¸ STRESS TEST RESULTS (Lower misses = Better Robustness):\")\n",
        "print(f\"  - Random Forest Missed: {rf_missed} / 100 attacks\")\n",
        "print(f\"  - CNN-LSTM Missed:      {dl_missed} / 100 attacks\")\n",
        "\n",
        "# Automatic Analysis\n",
        "diff = rf_missed - dl_missed\n",
        "if diff > 0:\n",
        "    print(f\"âœ… CONCLUSION: CNN-LSTM detected {diff} more attacks than Random Forest.\")\n",
        "    print(\"   (Deep Learning successfully saw through the obfuscation!)\")\n",
        "elif diff < 0:\n",
        "    print(f\"âš ï¸ CONCLUSION: Random Forest won by {-diff} points (Unusual, but possible).\")\n",
        "else:\n",
        "    print(\"âš–ï¸ CONCLUSION: Tie game. Both models struggled equally.\")"
      ],
      "metadata": {
        "id": "7afIVtiTJW6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Block 6: Final Visualization**\n",
        "To visually synthesize the results for the final report."
      ],
      "metadata": {
        "id": "gSdCiMZkXRpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Confusion Matrix Heatmaps\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Random Forest Heatmap\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=ax[0])\n",
        "ax[0].set_title(f\"Random Forest Confusion Matrix\\n(FPR: {fpr_rf:.2f}%)\")\n",
        "ax[0].set_xlabel(\"Predicted Label\")\n",
        "ax[0].set_ylabel(\"True Label\")\n",
        "\n",
        "# CNN-LSTM Heatmap\n",
        "sns.heatmap(cm_dl, annot=True, fmt='d', cmap='Reds', ax=ax[1])\n",
        "ax[1].set_title(f\"CNN-LSTM Confusion Matrix\\n(FPR: {fpr_dl:.2f}%)\")\n",
        "ax[1].set_xlabel(\"Predicted Label\")\n",
        "ax[1].set_ylabel(\"True Label\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. The Security-Performance Paradox: Latency vs. Robustness\n",
        "models = ['Random Forest', 'CNN-LSTM']\n",
        "latency = [lat_rf, lat_dl]\n",
        "missed_attacks = [rf_missed, dl_missed]\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Bar Chart for Latency (Left Axis)\n",
        "bars = ax1.bar(models, latency, color=['#3498db', '#e74c3c'], alpha=0.5, label='Inference Latency (ms)')\n",
        "ax1.set_ylabel('Latency (ms/query) - Lower is Faster', color='blue', fontsize=12)\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "ax1.set_ylim(0, max(latency) * 1.2) # Dynamic scaling for latency\n",
        "\n",
        "# Line Chart for Robustness (Right Axis)\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(models, missed_attacks, color='black', marker='o', markersize=10, linewidth=3, label='Attacks Missed')\n",
        "ax2.set_ylabel('Attacks Missed (out of 100) - Lower is Smarter', color='black', fontsize=12)\n",
        "ax2.set_ylim(0, 100) # Fixed scale for the 100-attack stress test\n",
        "\n",
        "# Aesthetics\n",
        "plt.title('The Security-Performance Paradox: Speed vs. Robustness', fontsize=14)\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add Legend\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax2.legend(lines + lines2, labels + labels2, loc='upper center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_EGAkZcSJW_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}